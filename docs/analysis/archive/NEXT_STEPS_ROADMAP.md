# ðŸš€ Next Steps Roadmap
**Piano Operativo per Integrazione Sistema Scraping**

**Data**: 2025-11-05
**Repository**: cookkie-real-estate-agent v3.0.0
**Target**: Sistema scraping intelligente production-ready

---

## ðŸ“š DOCUMENTI CREATI

Hai ora 3 documenti completi:

1. **`SCRAPING_INTEGRATION_PLAN.md`** (Main Plan)
   - Piano completo integrazione scraping
   - Architettura Playwright + Datapizza AI
   - PostgreSQL + Railway deployment
   - Tutte le 5 fasi di sviluppo

2. **`SESSION_PERSISTENCE_GUIDE.md`** (Session Management)
   - Alternative a Multilogin (â‚¬300/mese risparmiati)
   - Implementazione session persistence
   - Login automatici e cookie management
   - Security best practices

3. **`NEXT_STEPS_ROADMAP.md`** (Questo documento)
   - Checklist operativa
   - Comandi da eseguire
   - Ordine di implementazione

---

## ðŸŽ¯ OBIETTIVO FINALE

Sistema di scraping che:
- âœ… Estrae dati da Immobiliare.it, Casa.it, Idealista.it
- âœ… Usa browser automation reale (Playwright + Chromium)
- âœ… AI semantic extraction (Datapizza AI)
- âœ… Session persistence (login automatici)
- âœ… PostgreSQL su Railway.com
- âœ… API + Frontend dashboard
- âœ… Scheduled jobs (Celery)

---

## âš ï¸ PREREQUISITI CRITICI (GIORNO 1)

### STEP 0: Risolvere Blockers

Prima di iniziare lo sviluppo scraping, **DEVI** completare:

#### 0.1 âœ… Creare Prisma Schema (CRITICO)

**Problema**: `database/prisma/schema.prisma` NON ESISTE
**Impatto**: Blocca Next.js builds, Prisma Client, tutto il TypeScript/Node.js

**Soluzione**:

```bash
cd /home/user/cookkie-real-estate-agent/database/prisma

# Creare schema.prisma convertendo da SQLAlchemy models
# (vedi database/python/models.py per reference)
```

**Schema Template:**
```prisma
// database/prisma/schema.prisma

generator client {
  provider = "prisma-client-js"
  output   = "../../node_modules/.prisma/client"
}

datasource db {
  provider = "postgresql"  // For Railway (SQLite for local dev)
  url      = env("DATABASE_URL")
}

// ============================================================================
// CORE MODELS (from SQLAlchemy)
// ============================================================================

model UserProfile {
  id          String   @id @default(cuid())
  fullName    String
  email       String   @unique
  phone       String?
  agencyName  String?
  agencyVat   String?
  agencyAddress String?
  settings    Json     @default("{\"commissionPercent\":3.0}")
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  @@map("user_profile")
}

model Contact {
  id                String    @id @default(cuid())
  code              String    @unique
  entityType        String    @default("person")

  // Anagrafica
  fullName          String
  firstName         String?
  lastName          String?
  companyName       String?

  // Contatti
  primaryPhone      String?
  secondaryPhone    String?
  primaryEmail      String?
  secondaryEmail    String?

  // Indirizzo
  street            String?
  civic             String?
  city              String?
  province          String?
  zip               String?
  country           String    @default("Italia")
  latitude          Float?
  longitude         Float?

  // Dati Fiscali
  taxCode           String?
  vatNumber         String?
  birthDate         DateTime?
  nationality       String?

  // Privacy (GDPR)
  privacyFirstContact     Boolean   @default(false)
  privacyFirstContactDate DateTime?
  privacyExtended         Boolean   @default(false)
  privacyExtendedDate     DateTime?
  privacyMarketing        Boolean   @default(false)

  // Profilazione
  source            String?
  leadScore         Int?
  importance        String    @default("normal")

  // Budget
  budgetMin         Float?
  budgetMax         Float?

  // Status
  status            String    @default("active")
  lastContactDate   DateTime?
  notes             String?

  createdAt         DateTime  @default(now())
  updatedAt         DateTime  @updatedAt

  // Relations
  ownedProperties   Property[]  @relation("PropertyOwner")
  requests          Request[]
  activities        Activity[]
  matches           Match[]

  @@index([code])
  @@index([fullName])
  @@index([city])
  @@index([status])
  @@map("contacts")
}

model Building {
  id               String    @id @default(cuid())
  code             String    @unique

  // Indirizzo
  street           String
  civic            String
  city             String
  province         String
  zip              String?
  latitude         Float?
  longitude        Float?

  // Info Edificio
  yearBuilt        Int?
  totalFloors      Int?
  totalUnits       Int?
  hasElevator      Boolean   @default(false)
  condition        String?

  // Censimento
  lastSurveyDate   DateTime?
  nextSurveyDue    DateTime?
  unitsSurveyed    Int       @default(0)
  unitsInterested  Int       @default(0)

  notes            String?

  createdAt        DateTime  @default(now())
  updatedAt        DateTime  @updatedAt

  // Relations
  properties       Property[]
  activities       Activity[]

  @@map("buildings")
}

model Property {
  id               String    @id @default(cuid())
  code             String    @unique

  // Relations
  ownerContactId   String?
  owner            Contact?   @relation("PropertyOwner", fields: [ownerContactId], references: [id])
  buildingId       String?
  building         Building? @relation(fields: [buildingId], references: [id])

  // Status
  status           String    @default("draft")
  visibility       String    @default("public")

  // Fonte
  source           String
  sourceUrl        String?
  importDate       DateTime?
  verified         Boolean   @default(false)

  // Indirizzo
  street           String
  civic            String?
  internal         String?
  floor            String?
  city             String
  province         String
  zone             String?
  zip              String?
  latitude         Float
  longitude        Float

  // Tipo
  contractType     String
  propertyType     String
  propertyCategory String?

  // Dimensioni
  sqmCommercial    Float?
  sqmLivable       Float?
  rooms            Int?
  bedrooms         Int?
  bathrooms        Int?

  // Features
  hasElevator      Boolean   @default(false)
  hasParking       Boolean   @default(false)
  hasGarage        Boolean   @default(false)
  hasGarden        Boolean   @default(false)
  hasTerrace       Boolean   @default(false)

  // Caratteristiche
  condition        String?
  heatingType      String?
  energyClass      String?
  yearBuilt        Int?

  // Prezzi
  priceSale        Float?
  priceRentMonthly Float?

  // Marketing
  title            String?
  description      String?

  // Statistiche
  viewsCount       Int       @default(0)
  inquiriesCount   Int       @default(0)
  visitsCount      Int       @default(0)

  notes            String?
  internalNotes    String?

  createdAt        DateTime  @default(now())
  updatedAt        DateTime  @updatedAt

  // Relations
  matches          Match[]
  activities       Activity[]

  @@index([code])
  @@index([status])
  @@index([city])
  @@index([contractType])
  @@index([priceSale])
  @@map("properties")
}

model Request {
  id               String    @id @default(cuid())
  code             String    @unique

  // Relations
  contactId        String
  contact          Contact   @relation(fields: [contactId], references: [id])

  // Tipo & Stato
  requestType      String    @default("search_buy")
  status           String    @default("active")
  urgency          String    @default("medium")

  // Criteri
  contractType     String?
  searchCities     Json?     // JSON array
  searchZones      Json?     // JSON array
  propertyTypes    Json?     // JSON array

  // Budget
  priceMin         Float?
  priceMax         Float?

  // Dimensioni
  sqmMin           Float?
  sqmMax           Float?
  roomsMin         Int?
  roomsMax         Int?

  // Features Richieste
  requiresElevator Boolean   @default(false)
  requiresParking  Boolean   @default(false)
  requiresGarden   Boolean   @default(false)

  notes            String?

  createdAt        DateTime  @default(now())
  updatedAt        DateTime  @updatedAt
  expiresAt        DateTime?

  // Relations
  matches          Match[]
  activities       Activity[]

  @@index([code])
  @@index([status])
  @@index([contactId])
  @@map("requests")
}

model Match {
  id               String    @id @default(cuid())

  // Relations
  requestId        String
  request          Request   @relation(fields: [requestId], references: [id])
  propertyId       String
  property         Property  @relation(fields: [propertyId], references: [id])
  contactId        String?
  contact          Contact?  @relation(fields: [contactId], references: [id])

  // Scoring
  scoreTotal       Int
  scoreLocation    Int?
  scorePrice       Int?
  scoreSize        Int?
  scoreFeatures    Int?

  // Stato
  status           String    @default("suggested")

  // Feedback
  clientReaction   String?
  rejectionReason  String?
  clientNotes      String?

  // Azioni
  sentDate         DateTime?
  viewedDate       DateTime?
  visitedDate      DateTime?

  agentNotes       String?

  createdAt        DateTime  @default(now())
  updatedAt        DateTime  @updatedAt

  @@index([requestId])
  @@index([propertyId])
  @@index([scoreTotal])
  @@index([status])
  @@map("matches")
}

model Activity {
  id               String    @id @default(cuid())

  // Polymorphic Relations
  contactId        String?
  contact          Contact?   @relation(fields: [contactId], references: [id])
  propertyId       String?
  property         Property?  @relation(fields: [propertyId], references: [id])
  requestId        String?
  request          Request?   @relation(fields: [requestId], references: [id])
  buildingId       String?
  building         Building?  @relation(fields: [buildingId], references: [id])

  // Tipo & Stato
  activityType     String
  status           String    @default("scheduled")
  priority         String    @default("normal")

  // Temporizzazione
  scheduledAt      DateTime?
  completedAt      DateTime?
  dueDate          DateTime?

  // Contenuto
  title            String
  description      String?
  outcome          String?
  details          Json      @default("{}")

  notes            String?

  createdAt        DateTime  @default(now())
  updatedAt        DateTime  @updatedAt

  @@index([contactId])
  @@index([propertyId])
  @@index([activityType])
  @@index([status])
  @@index([scheduledAt])
  @@map("activities")
}

// ============================================================================
// SCRAPING MODELS (NEW)
// ============================================================================

model ScrapingJob {
  id                   String    @id @default(cuid())
  siteUrl              String
  siteName             String    // immobiliare_it, casa_it, idealista_it
  siteType             String    // portal, crm, catasto, generic

  // Configuration
  searchConfig         Json      // Search parameters
  extractionConfig     Json      // What data to extract
  scheduleConfig       Json      // Cron schedule

  // Credentials (encrypted)
  credentialsEncrypted String?

  // Browser Profile
  browserProfile       Json?     // User agent, fingerprint, cookies
  proxyConfig          Json?     // Proxy settings

  // Status
  status               String    @default("active") // active, paused, failed
  lastRunAt            DateTime?
  nextRunAt            DateTime?
  lastSuccess          DateTime?
  successCount         Int       @default(0)
  failureCount         Int       @default(0)

  createdAt            DateTime  @default(now())
  updatedAt            DateTime  @updatedAt

  // Relations
  extractions          ScrapedData[]
  sessions             ScrapingSession[]

  @@index([status, nextRunAt])
  @@index([siteName])
  @@map("scraping_jobs")
}

model ScrapedData {
  id                  String    @id @default(cuid())
  jobId               String
  job                 ScrapingJob @relation(fields: [jobId], references: [id], onDelete: Cascade)

  // Source
  sourceUrl           String
  contentHash         String    @unique // Dedup

  // Data
  dataType            String    // property, contact, document
  rawData             Json      // Raw extracted data
  processedData       Json?     // After AI processing

  // Quality
  confidenceScore     Float?    // AI confidence (0-1)
  validationStatus    String    @default("pending") // pending, valid, invalid
  validationErrors    Json?

  // Media
  images              Json?     // Array of image URLs
  documents           Json?     // Array of document paths

  extractedAt         DateTime  @default(now())

  @@index([jobId, extractedAt])
  @@index([contentHash])
  @@index([dataType])
  @@map("scraped_data")
}

model ScrapingSession {
  id                  String    @id @default(cuid())

  // Identity
  profileName         String    @unique  // "immobiliare_roma_agent1"
  portalName          String              // "immobiliare_it"

  // Session Data
  cookies             Json                // Browser cookies array
  localStorage        Json?               // localStorage key-value pairs
  sessionStorage      Json?               // sessionStorage key-value pairs

  // Browser Fingerprint
  userAgent           String
  viewport            Json                // { width: 1920, height: 1080 }
  timezone            String   @default("Europe/Rome")
  locale              String   @default("it-IT")

  // Authentication
  isAuthenticated     Boolean  @default(false)
  loginData           Json?               // Encrypted credentials
  lastLoginAt         DateTime?

  // Proxy (optional)
  proxyServer         String?
  proxyUsername       String?
  proxyPassword       String?  // Encrypted

  // Status
  isValid             Boolean  @default(true)
  lastUsedAt          DateTime @default(now())
  expiresAt           DateTime?

  // Metadata
  createdAt           DateTime @default(now())
  updatedAt           DateTime @updatedAt

  // Stats
  useCount            Int      @default(0)
  successCount        Int      @default(0)
  failureCount        Int      @default(0)

  // Relations
  jobId               String?
  job                 ScrapingJob? @relation(fields: [jobId], references: [id])

  @@index([portalName, isValid])
  @@index([profileName])
  @@map("scraping_sessions")
}
```

**Dopo la creazione:**

```bash
# Generate Prisma Client
npx prisma generate

# Push to database (SQLite local dev)
npx prisma db push

# Verify
npx prisma studio  # Should open GUI
```

**Output Atteso**: Prisma Client generato, TypeScript builds funzionano

---

## ðŸ“… TIMELINE SVILUPPO

### Settimana 1: Setup Infrastruttura
- [x] Prisma schema creato âœ…
- [ ] PostgreSQL configurato per Railway
- [ ] Playwright installato
- [ ] Browser manager implementato
- [ ] Session manager implementato

### Settimana 2: Scraper Immobiliare.it
- [ ] Scraper base con Playwright
- [ ] Parsing search results
- [ ] AI semantic extraction (Datapizza)
- [ ] Database integration
- [ ] Session persistence testing

### Settimana 3: Task Scheduling & API
- [ ] Celery + Redis setup
- [ ] Scraping tasks implementati
- [ ] FastAPI endpoints
- [ ] Database repository
- [ ] Scheduled jobs configurati

### Settimana 4: Frontend & Testing
- [ ] Frontend dashboard
- [ ] Job management UI
- [ ] Unit tests
- [ ] Integration tests
- [ ] E2E testing

### Settimana 5: Deployment & Launch
- [ ] Railway deployment
- [ ] PostgreSQL migration
- [ ] Production testing
- [ ] Monitoring setup
- [ ] Launch graduale

---

## ðŸ› ï¸ COMANDI QUICK START

### Setup Ambiente

```bash
cd /home/user/cookkie-real-estate-agent

# 1. Creare Prisma schema (vedi sopra)
# 2. Install dependencies
npm run install:all

# 3. Setup environment files
cp config/backend.env.example backend/.env
cp config/frontend.env.example frontend/.env.local
cp config/ai_tools.env.example ai_tools/.env

# 4. Edit .env files con le tue API keys
# GOOGLE_API_KEY=your_key
# DATABASE_URL=...

# 5. Generate Prisma Client
npm run prisma:generate

# 6. Push database schema
npm run prisma:push

# 7. (Optional) Seed database
npm run prisma:seed
```

### Install Playwright (Scraping)

```bash
cd scraping

# Add to requirements.txt
echo "playwright==1.47.0" >> requirements.txt
echo "playwright-stealth==1.0.3" >> requirements.txt

# Install
pip install -r requirements.txt

# Install Chromium browser
playwright install chromium
playwright install-deps
```

### Start Development

```bash
# Terminal 1: Frontend
cd frontend
npm run dev  # Port 3000

# Terminal 2: Backend API
cd backend
npm run dev  # Port 3001

# Terminal 3: AI Tools
cd ai_tools
python main.py  # Port 8000

# Terminal 4: Celery Worker (dopo Redis setup)
cd ai_tools
celery -A app.celery_app worker --loglevel=info
```

---

## ðŸŽ¯ PRIORITY CHECKLIST

### Immediate (Oggi)
- [ ] Creare `database/prisma/schema.prisma`
- [ ] Testare Prisma Client generation
- [ ] Verificare Next.js builds funzionano

### High Priority (Questa Settimana)
- [ ] Configurare PostgreSQL per Railway
- [ ] Installare Playwright + Chromium
- [ ] Implementare BrowserManager base
- [ ] Implementare SessionManager
- [ ] Creare scraper Immobiliare.it base

### Medium Priority (Prossime 2 Settimane)
- [ ] Database integration scrapers
- [ ] Celery task queue setup
- [ ] FastAPI scraping endpoints
- [ ] Frontend dashboard base

### Low Priority (Prossimo Mese)
- [ ] Casa.it e Idealista.it scrapers
- [ ] Advanced AI features
- [ ] Monitoring dashboard
- [ ] Production optimization

---

## ðŸ’¾ POSTGRESQL MIGRATION

### Local Development (SQLite)

```bash
# database/prisma/schema.prisma
datasource db {
  provider = "sqlite"
  url      = "file:./dev.db"
}
```

### Production (Railway PostgreSQL)

```bash
# 1. Create Railway project
# 2. Add PostgreSQL service
# 3. Get DATABASE_URL from Railway dashboard

# 4. Update schema.prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

# 5. Generate migration
npx prisma migrate dev --name init

# 6. Deploy to Railway
npx prisma migrate deploy
```

---

## ðŸš€ RAILWAY DEPLOYMENT

### 1. Creare File di Configurazione

**`railway.json`:**
```json
{
  "$schema": "https://railway.app/railway.schema.json",
  "build": {
    "builder": "NIXPACKS"
  },
  "deploy": {
    "startCommand": "npm run start:all",
    "healthcheckPath": "/health",
    "restartPolicyType": "ON_FAILURE"
  }
}
```

**`nixpacks.toml`:**
```toml
[phases.setup]
nixPkgs = ["nodejs-18_x", "python311", "postgresql"]

[phases.install]
cmds = [
  "npm run install:all",
  "cd ai_tools && pip install -r requirements.txt",
  "playwright install chromium"
]

[start]
cmd = "npm run start:all"
```

### 2. Environment Variables su Railway

```bash
DATABASE_URL=${{ POSTGRES.DATABASE_URL }}
GOOGLE_API_KEY=your_key
NODE_ENV=production
PORT=3000
```

### 3. Deploy

```bash
# Push to GitHub
git add .
git commit -m "Add scraping system"
git push

# Railway auto-deploys from GitHub
```

---

## ðŸ“Š MONITORING & MAINTENANCE

### Weekly Tasks
- [ ] Check scraping job success rate
- [ ] Review failed jobs logs
- [ ] Update scrapers if portal layouts changed
- [ ] Monitor database size growth

### Monthly Tasks
- [ ] Analyze data quality metrics
- [ ] Review cost efficiency (Railway, Google AI)
- [ ] Update AI prompts if needed
- [ ] Check session persistence stats

---

## ðŸ†˜ TROUBLESHOOTING

### "Prisma Client not found"
```bash
npm run prisma:generate
```

### "Playwright browser not found"
```bash
cd scraping
playwright install chromium
```

### "Next.js build fails"
â†’ Prisma schema missing, create it first

### "Railway deployment fails"
â†’ Check build logs for missing dependencies

### "Scraper gets blocked"
â†’ Add residential proxies + increase delays

---

## ðŸ“ž SUPPORTO

### Documentazione Locale
- `/docs/analysis/SCRAPING_INTEGRATION_PLAN.md` - Piano completo
- `/docs/analysis/SESSION_PERSISTENCE_GUIDE.md` - Session management
- `/docs/analysis/NEXT_STEPS_ROADMAP.md` - Questo documento

### External Resources
- [Playwright Docs](https://playwright.dev/python/)
- [Datapizza AI Docs](https://datapizza.tech/)
- [Railway Docs](https://docs.railway.app/)
- [Prisma Docs](https://www.prisma.io/docs/)

---

## âœ… DEFINITION OF DONE

Prima di considerare il progetto completo:

- [ ] âœ… Prisma schema exists e Prisma Client generato
- [ ] âœ… PostgreSQL configurato su Railway
- [ ] âœ… Playwright + Chromium installati e funzionanti
- [ ] âœ… Scraper Immobiliare.it estrae dati con >95% success rate
- [ ] âœ… Session persistence funziona (login saved between runs)
- [ ] âœ… Datapizza AI estrae campi strutturati
- [ ] âœ… Database save con deduplication funziona
- [ ] âœ… Celery tasks schedulano scraping automaticamente
- [ ] âœ… FastAPI endpoints rispondono correttamente
- [ ] âœ… Frontend dashboard visualizza jobs
- [ ] âœ… Test suite >80% coverage
- [ ] âœ… Railway deployment funziona end-to-end
- [ ] âœ… Documentazione completa e aggiornata
- [ ] âœ… Monitoring configurato (logs, metrics)

---

## ðŸŽ‰ SUCCESSO FINALE

Quando tutto Ã¨ completo, avrai:

âœ… Sistema scraping enterprise-grade
âœ… Browser automation reale (Playwright)
âœ… AI semantic extraction (Datapizza AI)
âœ… Session persistence (no Multilogin needed)
âœ… PostgreSQL su Railway (production-ready)
âœ… API + Frontend dashboard
âœ… Scheduled jobs automatici
âœ… ~â‚¬300/mese risparmiati vs piano originale

**ROI**: Positivo entro 3 mesi
**Time Saving**: 80% riduzione data entry manuale
**Data Quality**: >99% accuracy

---

**Next Action**: Inizia con STEP 0.1 (Creare Prisma Schema) â†’ Poi segui checklist sequenzialmente

**Buon lavoro! ðŸš€**

---

**Documento creato**: 2025-11-05
**Autore**: Claude Code
**Repository**: /home/user/cookkie-real-estate-agent
